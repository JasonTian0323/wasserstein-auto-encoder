{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dim_h'] = 40            # factor controlling size of hidden layers\n",
    "args['n_channel'] = 1         # number of channels in the input data (MNIST is 1, aka greyscale)\n",
    "args['n_z'] = 20              # number of dimensions in latent space. \n",
    "args['sigma'] = 1.0           # variance in n_z\n",
    "args['lambda'] = 0.01           # hyper param for weight of discriminator loss\n",
    "args['lr'] = 0.0002           # learning rate for Adam optimizer\n",
    "args['epochs'] = 50            # how many epochs to run for\n",
    "args['batch_size'] = 256      # batch size for SGD\n",
    "args['save'] = False          # save weights at each epoch of training if True\n",
    "args['train'] = False         # train networks if True, else load networks from saved weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all encoder and decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class EncoderD(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(EncoderD, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "## create encoder model and decoder model\n",
    "class EncoderS(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(EncoderS, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc1 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "        self.fc2 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "        \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = h.squeeze()\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final test loss for FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set FMNIST test set\n",
    "dataset = 'FashionMNIST'\n",
    "testset = datasets.FashionMNIST(\n",
    "    root='./FMNIST/',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# load the full validation set as batch size\n",
    "test_loader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=10000,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for all models\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE final reconstruction loss 0.008279001340270042 on FashionMNIST\n"
     ]
    }
   ],
   "source": [
    "# standard AE\n",
    "encoder, decoder = EncoderD(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/AE_encoder-best_fmnist.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/AE_decoder-best_fmnist.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('AE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE final reconstruction loss 0.013607991859316826 on FashionMNIST\n"
     ]
    }
   ],
   "source": [
    "# VAE\n",
    "encoder, decoder = EncoderS(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/VAE_encoder-best_fmnist.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/VAE_decoder-best_fmnist.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat, mu, logvar = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('VAE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE final reconstruction loss 0.009220210835337639 on FashionMNIST\n"
     ]
    }
   ],
   "source": [
    "#WAE\n",
    "encoder, decoder = EncoderD(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/WAEgan_encoder-best_fmnist.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/WAEgan_decoder-best_fmnist.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('WAE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set FMNIST test set\n",
    "dataset = 'MNIST'\n",
    "testset = datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# load the full validation set as batch size\n",
    "test_loader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=10000,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE final reconstruction loss 0.004790022969245911 on MNIST\n"
     ]
    }
   ],
   "source": [
    "# standard AE\n",
    "encoder, decoder = EncoderD(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/AE_encoder-best.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/AE_decoder-best.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('AE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE final reconstruction loss 0.009450603276491165 on MNIST\n"
     ]
    }
   ],
   "source": [
    "# VAE\n",
    "encoder, decoder = EncoderS(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/VAE_encoder-best.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/VAE_decoder-best.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat, mu, logvar = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('VAE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAE final reconstruction loss 0.007124471943825483 on MNIST\n"
     ]
    }
   ],
   "source": [
    "#WAE\n",
    "encoder, decoder = EncoderD(args), Decoder(args)\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# load encoder and decoder weights from checkpoint\n",
    "enc_checkpoint = torch.load('save/WAEgan_encoder-best.pth')\n",
    "encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "dec_checkpoint = torch.load('save/WAEgan_decoder-best.pth')\n",
    "decoder.load_state_dict(dec_checkpoint)\n",
    "\n",
    "for images, _ in test_loader:\n",
    "    z_hat = encoder(images)\n",
    "    x_hat = decoder(z_hat)\n",
    "    test_recon_loss = criterion(x_hat, images)\n",
    "    print('WAE final reconstruction loss {} on {}'.format(test_recon_loss.data.item(), dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
