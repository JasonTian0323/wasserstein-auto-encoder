{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to the Wasserstein Auto-encoder\n",
    "\n",
    "-------\n",
    "\n",
    "## Authors\n",
    "Joel Dapello<br>\n",
    "Michael Sedelmeyer<br>\n",
    "Wenjun Yan\n",
    "\n",
    "-------\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "\n",
    "Table of contents with markdown hyperlinks to each section of the notebook\n",
    "\n",
    "1. [Motivation and background](#intro)\n",
    "\n",
    "1. [Conceptual foundations](#concepts)\n",
    "\n",
    "1. [Mathematics and algorithms](#details)\n",
    "\n",
    "1. [Comparing results on MNIST](#mnist)\n",
    "\n",
    "1. [Comparing results on FashionMNIST](#fmnist)\n",
    "\n",
    "1. [Conclusions and further analysis](#conclusion)\n",
    "\n",
    "1. [References and further reading](#sources)\n",
    "\n",
    "\n",
    "- [Appendices: PyTorch Implementation](#appendix)\n",
    "    - [Appendix A: Auto-encoder](#ae)\n",
    "    - [Appendix B: Variational auto-encoder](#vae)\n",
    "    - [Appendix C: Wasserstein auto-encoder](#wae)\n",
    "    - [Appendix C: Plotting functions](#plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Motivation and background\n",
    "[return to top](#top)\n",
    "\n",
    "Designing generative models capabale of capturing the structure of very high dimensional data is a standing problem in the field of statistical modeling. One class of models that have proved effective for this task is the auto-encoder (AE). AEs are neural network based models that assume the high dimensional data being modeled can be reduced to a lower dimensional manifold, defined on a space of latent variables. To do this, the AE defines an encoder network $Q$ which maps a high dimensional input to a low dimensional latent space $Z$, and a generator network $G$ which maps $Z$ back to the high dimensional input space. The whole system is trained end to end with stochastic gradient descent, where, in the case of the vanilla AE, the cost function is designed to minimize the distance between the training data $X$ and it's reconstruction, $\\hat{X} = G(Q(X))$. While the standard AE is quite effective at learning a low dimensional representation of the training data, it is prone to overfitting, and typically fails as a generative model. This is because with no constraint on the shape of the learned representation in latent space, it is unclear how to effectively sample from $Z$ -- passing randomly draw latent codes which are far from the those that G has learned to decode often lead to the generation of nonsense.\n",
    "\n",
    "The well-known variational auto-encoder (VAE) (Kingma & Welling, 2014) was introduced as a solution to this problem. The VAE builds on the AE frame work with a modified cost function designed to maximize the evidence lower bound between the model and target distribution. This effectively introduces a regularization penalty which pushes $Q_z=Q(Z|X=x)$ to match a specified prior distribution, $P_z$. Thus, the VAE functions as a much more powerful generative model than the standard AE, because samples drawn from the $P_z$ are in a range that the $G$ has learned to generate from. Unfortunately, while the VAE performs admirably on simple datasets such as MNIST, with more complex datasets the VAE tends to recreate blurred samples.\n",
    "\n",
    "In 2018 with the Internation Conference on Learning Representations paper \"Wasserstein Auto-Encoders\", the authors Tolstikhin et. al. propose the Wasserstein auto-encoder (WAE) as a new algorithm for building a latent-variable-based generative model. This new addition to the family of regularized auto-encoders aims to minimize the optimal transport cost, $\\mathcal{D}_Z(Q_Z,P_Z)$ (Villani, 2003) formulated as the Wasserstein distance between the model distribution $Q_Z$ and the target $P_Z$ distribution. This can be thought of intuitively as the cost to transform one distribution into another, and leads to a different regularization penalty than that of the VAE. The WAE regularizer encourages the full encoded training distribution to form a continuous mixturing matching the $P_Z$ rather than individual samples as happens in the case of the VAE (see [Figure 1](#fig1)). For this reason, the WAE shares many of the properties of VAEs, while generating better quality samples due to a better disentangling of the latent space due to the optimal transport penalty.\n",
    "\n",
    "In this tutorial, we implement the generative adversarial network (GAN) formulation of WAE (WAEgan). The WAEgan uses the Kantorovich-Rubinstein duality (CITE), expressed as an adversarial objective on the latent space. Specifically, the WAEgan implements a discriminator network $D$ in the latent space $Z$ trying to differentiate between samples drawn from $P_Z$ and samples drawn from $Q_Z$, essentially setting $\\mathcal{D}_Z(Q_Z,P_Z)=D(Q_Z,P_Z)$, and forcing $Q$ to learn to generate latent codes that fool the discriminator $D$. In addition to implementing the WAEgan, we implement a VAE and vanilla AE as well. We choose this approach because, to better understand the WAE and its benefits, it is important to consider WAE within the context of these two preceeding and well-established algorithms. This approach provides a more intuitive understanding of the results by demonstrating side-by-side comparisons of each algorithm applied to the popular MNIST (CITE) and FashionMNIST (CITE) datasets with convolutional nueral network (CNN) implementations in PyTorch. \n",
    "\n",
    "<a id=\"fig1\"></a>\n",
    "**Figure 1:** Conceptual comparison of AE reconstruction methods (after Tolsikhin, et.al 2018). All three algorithms map inputs $x \\in X$ to a latent code $z \\in Z$ and then attempt to reconstruct $\\hat{x}=G(z)$. The AE places no regularization penalty on $Z$, while the VAE and WAE use Kullbackâ€“Leibler divergence (KLD) and optimal transport cost respectively to penalize divergence of $Q_Z$ from the shape of the prior, $P_Z$. While KLD forces Q(Z|X=x) to match $P_Z$, the optimal transport cost enforces the continuous mixture $Q_z:=\\int Q(Z|X) dP_x$ to match $P_Z$.\n",
    "\n",
    "![alt text](https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/figure%201%20-%20reconstruction.png?raw=true \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"details\"></a>\n",
    "## Mathematics and algorithms\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we provide the mathematical detail and algorithmic differences between each method, paying extra attention to WAE and how it varies from VAE.\n",
    "\n",
    "**latex to include:**\n",
    "1. notational algorithms\n",
    "1. loss function detail\n",
    "1. mathematical representation of the reparameterization trick\n",
    "\n",
    "**images to include:**\n",
    "1. A small graphical representation of the reparameterization trick (small and simple node/edge plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mnist\"></a>\n",
    "## Comparing results on MNIST\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we specify the parameters used in our model and provide plots and metrics and written interpretation describing the training results and latent space representations of our algorithms on MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. Sample of 5 original MNIST images and corresponding decoded images for AE, VAE, and WAE on separate rows\n",
    "1. Latent space linear interpolation results of each model, pixel space vs AE vs VAE vs WAE on separate rows\n",
    "1. tSNE or PCA representation of pixel space vs latent space for each model to demonstrate differences\n",
    "1. table summarizing comparative loss (and if possible FID results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fmnist\"></a>\n",
    "## Comparing results on FashionMNIST\n",
    "[return to top](#top)\n",
    "\n",
    "Same as above for MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. same as above for MNIST, but probably smaller and with fewer examples if results demonstrate similar characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusions and further analysis\n",
    "[return to top](#top)\n",
    "\n",
    "Here we summarize our conclusions given MNIST and FMNIST, but also describe other dataset we may want to run as comparison (e.g. celeb faces for representation on a low manifold surface such a faces, RNA expression data for investigation of a novel application of WAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## References and Further Reading\n",
    "[return to top](#top)\n",
    "\n",
    "Cite the papers, repos, datasets, and blogs we used in our analysis, as well as any other resources we want to direct our readers toward\n",
    "\n",
    "1. VAE paper\n",
    "1. WAE paper\n",
    "1. PyTorch/resources implementation of VAE\n",
    "1. AE paper?\n",
    "1. MNIST\n",
    "1. FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix\"></a>\n",
    "## Appendices: PyTorch Implementation\n",
    "[return to top](#top)\n",
    "\n",
    "- The Appendix is where we lay out and run our PyTorch code, each model is separated among sub-appendices\n",
    "- We should output our most important plots to png (saved on GitHub) so we can display them via markdown img link at the appropriate locations in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Set parameter args\n",
    "# load data train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ae\"></a>\n",
    "### Appendix A: Auto-encoder \n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "### Appendix B: Variational auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wae\"></a>\n",
    "### Appendix C: Wasserstein auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "### Appendix D: Plotting functions\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BOTH ALGORITHMS SHOULD PROBABLY BE REWRITTEN USING LATEX IN A WAY THAT TAKES UP LESS VERTICAL SPACE**\n",
    "\n",
    "<a id=\"algo2\"></a>\n",
    "**Algorithm 2:** Wassertein auto-encoder with GAN-based penalty (WAE-GAN) pseudocode\n",
    "\n",
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$, and latent discriminator $D_{\\gamma}$.\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{z_1, \\dotsc , z_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $D_{\\gamma}$ by ascending:\n",
    "$$\\frac{\\lambda}{n}\\sum_{i=1}^n log \\; D_{\\gamma}(z_i) + log (1-D_{\\gamma}(\\tilde{z}_i))$$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i)) - \\lambda \\cdot log\\;D_{\\gamma}(\\tilde{z}_i)$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algo1\"></a>\n",
    "**Algorithm 1:** Variational auto-encoder pseudocode for computing a stochastic graient using the estimator\n",
    "\n",
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters for the encoder $Q_{\\phi}$ and decoder $G_{\\theta}$\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{\\epsilon_1, \\dotsc , \\epsilon_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i))$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
