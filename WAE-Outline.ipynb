{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introductory Tutorial On the Wasserstein Auto-encoder\n",
    "\n",
    "-------\n",
    "\n",
    "## Authors\n",
    "Joel Dapello<br>\n",
    "Michael Sedelmeyer<br>\n",
    "Wenjun Yan\n",
    "\n",
    "-------\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "\n",
    "Table of contents with markdown hyperlinks to each section of the notebook\n",
    "\n",
    "1. [Motivation and background](#intro)\n",
    "\n",
    "1. [Conceptual foundations](#concepts)\n",
    "\n",
    "1. [Mathematics and algorithms](#details)\n",
    "\n",
    "1. [Comparing results on MNIST](#mnist)\n",
    "\n",
    "1. [Comparing results on FashionMNIST](#fmnist)\n",
    "\n",
    "1. [Conclusions and further analysis](#conclusion)\n",
    "\n",
    "1. [References and further reading](#sources)\n",
    "\n",
    "\n",
    "- [Appendices: PyTorch Implementation](#appendix)\n",
    "    - [Appendix A: Auto-encoder](#ae)\n",
    "    - [Appendix B: Variational auto-encoder](#vae)\n",
    "    - [Appendix C: Wasserstein auto-encoder](#wae)\n",
    "    - [Appendix C: Plotting functions](#plots)\n",
    "\n",
    "----------\n",
    "\n",
    "<a id=\"intro\"></a>\n",
    "## Motivation and background\n",
    "[return to top](#top)\n",
    "\n",
    "Explicitly cite the primary paper we are referencing and provide a high-level motivation for WAE\n",
    "\n",
    "**images to include:**\n",
    "1. side-by-side plot similar to the one found in the WAE paper showing AE vs VAE vs WAE reconstruction (this chart may be better suited for next section, but that may depend on how we summarize the motivation for WAE in this section)\n",
    "\n",
    "<a id=\"concepts\"></a>\n",
    "## Conceptual foundations\n",
    "[return to top](#top)\n",
    "\n",
    "Provide and illustrate the conceptual foundations and intuition for AE vs VAE vs WAE\n",
    "\n",
    "This should probably be written linearly, using the AE as our base example, and then adding elements of VAE, and then WAE, demonstrating the evolution of our chosen method.\n",
    "\n",
    "**images to include:**\n",
    "1. simplified CNN diagram demonstrating the generic encoding, latent space bottleneck, and decoding networks as a left-to-right process-flow (similary to either the full-connected or CNN-representative diagram)\n",
    "2. A further simplified version of the CNN diagram, with emphasis on the mechanics of the latent space of each method (i.e. similar to [this sort of image](http://kvfrans.com/content/images/2016/08/vae.jpg), but with MNIST digit images at either end) \n",
    "\n",
    "<a id=\"details\"></a>\n",
    "## Mathematics and algorithms\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we provide the mathematical detail and algorithmic differences between each method, paying extra attention to WAE and how it varies from VAE.\n",
    "\n",
    "**latex to include:**\n",
    "1. notational algorithms\n",
    "1. loss function detail\n",
    "1. mathematical representation of the reparameterization trick\n",
    "\n",
    "**images to include:**\n",
    "1. A small graphical representation of the reparameterization trick (small and simple node/edge plot)\n",
    "\n",
    "\n",
    "<a id=\"mnist\"></a>\n",
    "## Comparing results on MNIST\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we specify the parameters used in our model and provide plots and metrics and written interpretation describing the training results and latent space representations of our algorithms on MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. Sample of 5 original MNIST images and corresponding decoded images for AE, VAE, and WAE on separate rows\n",
    "1. Latent space linear interpolation results of each model, pixel space vs AE vs VAE vs WAE on separate rows\n",
    "1. tSNE or PCA representation of pixel space vs latent space for each model to demonstrate differences\n",
    "1. table summarizing comparative loss (and if possible FID results)\n",
    "\n",
    "\n",
    "<a id=\"fmnist\"></a>\n",
    "## Comparing results on FashionMNIST\n",
    "[return to top](#top)\n",
    "\n",
    "Same as above for MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. same as above for MNIST, but probably smaller and with fewer examples if results demonstrate similar characteristics\n",
    "\n",
    "<a id=\"conclusion\"></a>\n",
    "## Conclusions and further analysis\n",
    "[return to top](#top)\n",
    "\n",
    "Here we summarize our conclusions given MNIST and FMNIST, but also describe other dataset we may want to run as comparison (e.g. celeb faces for representation on a low manifold surface such a faces, RNA expression data for investigation of a novel application of WAE)\n",
    "\n",
    "<a id=\"conclusion\"></a>\n",
    "## References and Further Reading\n",
    "[return to top](#top)\n",
    "\n",
    "Cite the papers, repos, datasets, and blogs we used in our analysis, as well as any other resources we want to direct our readers toward\n",
    "\n",
    "1. VAE paper\n",
    "1. WAE paper\n",
    "1. PyTorch/resources implementation of VAE\n",
    "1. AE paper?\n",
    "1. MNIST\n",
    "1. FashionMNIST\n",
    "\n",
    "<a id=\"appendix\"></a>\n",
    "## Appendices: PyTorch Implementation\n",
    "[return to top](#top)\n",
    "\n",
    "- The Appendix is where we lay out and run our PyTorch code, each model is separated among sub-appendices\n",
    "- We should output our most important plots to png (saved on GitHub) so we can display them via markdown img link at the appropriate locations in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Set parameter args\n",
    "# load data train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ae\"></a>\n",
    "### Appendix A: Auto-encoder \n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "### Appendix B: Variational auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wae\"></a>\n",
    "### Appendix C: Wasserstein auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "### Appendix D: Plotting functions\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$, and latent discriminator $D_{\\gamma}$.\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{z_1, \\dotsc , z_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $D_{\\gamma}$ by ascending:\n",
    "$$\\frac{\\lambda}{n}\\sum_{i=1}^n log \\; D_{\\gamma}(z_i) + log (1-D_{\\gamma}(\\tilde{z}_i))$$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i)) - \\lambda \\cdot log\\;D_{\\gamma}(\\tilde{z}_i)$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters for the encoder $Q_{\\phi}$ and decoder $G_{\\theta}$\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{\\epsilon_1, \\dotsc , \\epsilon_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i))$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
