{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to the Wasserstein Auto-encoder\n",
    "\n",
    "-------\n",
    "\n",
    "## Authors\n",
    "Joel Dapello<br>\n",
    "Michael Sedelmeyer<br>\n",
    "Wenjun Yan\n",
    "\n",
    "-------\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "\n",
    "Table of contents with markdown hyperlinks to each section of the notebook\n",
    "\n",
    "1. [Motivation and background](#intro)\n",
    "\n",
    "1. [Conceptual foundations](#concepts)\n",
    "\n",
    "1. [Mathematics and algorithms](#details)\n",
    "\n",
    "1. [Comparing results on MNIST](#mnist)\n",
    "\n",
    "1. [Comparing results on FashionMNIST](#fmnist)\n",
    "\n",
    "1. [Conclusions and further analysis](#conclusion)\n",
    "\n",
    "1. [References and further reading](#sources)\n",
    "\n",
    "\n",
    "- [Appendices: PyTorch Implementation](#appendix)\n",
    "    - [Appendix A: Auto-encoder](#ae)\n",
    "    - [Appendix B: Variational auto-encoder](#vae)\n",
    "    - [Appendix C: Wasserstein auto-encoder](#wae)\n",
    "    - [Appendix C: Plotting functions](#plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Motivation and background\n",
    "[return to top](#top)\n",
    "\n",
    "Designing generative models capabale of capturing the structure of very high dimensional data is a standing problem in the field of statistical modeling. One class of models that have proved effective for this task is the auto-encoder (AE). AEs are neural network based models that assume the high dimensional data being modeled can be reduced to a lower dimensional manifold, defined on a space of latent variables. To do this, the AE defines an encoder network $Q$ which maps a high dimensional input to a low dimensional latent space $Z$, and a generator network $G$ which maps $Z$ back to the high dimensional input space. The whole system is trained end to end with stochastic gradient descent, where, in the case of the vanilla AE, the cost function is designed to minimize the distance between the training data $X$ and it's reconstruction, $\\hat{X} = G(Q(X))$. While the standard AE is quite effective at learning a low dimensional representation of the training data, it is prone to overfitting, and typically fails as a generative model. This is because with no constraint on the shape of the learned representation in latent space, it is unclear how to effectively sample from $Z$ -- passing randomly draw latent codes which are far from the those that G has learned to decode often lead to the generation of nonsense.\n",
    "\n",
    "The well-known variational auto-encoder (VAE) (Kingma & Welling, 2014) was introduced as a solution to this problem. The VAE builds on the AE frame work with a modified cost function designed to maximize the evidence lower bound between the model and target distribution. This effectively introduces a regularization penalty which pushes $Q_z=Q(Z|X=x)$ to match a specified prior distribution, $P_z$. Thus, the VAE functions as a much more powerful generative model than the standard AE, because samples drawn from the $P_z$ are in a range that the $G$ has learned to generate from. Unfortunately, while the VAE performs admirably on simple datasets such as MNIST, with more complex datasets the VAE tends to recreate blurred samples.\n",
    "\n",
    "In 2018 with the Internation Conference on Learning Representations paper \"Wasserstein Auto-Encoders\", the authors Tolstikhin et. al. propose the Wasserstein auto-encoder (WAE) as a new algorithm for building a latent-variable-based generative model. This new addition to the family of regularized auto-encoders aims to minimize the optimal transport cost, $\\mathcal{D}_Z(Q_Z,P_Z)$ (Villani, 2003) formulated as the Wasserstein distance between the model distribution $Q_Z$ and the target $P_Z$ distribution. This can be thought of intuitively as the cost to transform one distribution into another, and leads to a different regularization penalty than that of the VAE. The WAE regularizer encourages the full encoded training distribution to form a continuous mixturing matching the $P_Z$ rather than individual samples as happens in the case of the VAE (see [Figure 1](#fig1)). For this reason, the WAE shares many of the properties of VAEs, while generating better quality samples due to a better disentangling of the latent space due to the optimal transport penalty.\n",
    "\n",
    "In this tutorial, we implement the generative adversarial network (GAN) formulation of WAE (WAEgan). The WAEgan uses the Kantorovich-Rubinstein duality (CITE), expressed as an adversarial objective on the latent space. Specifically, the WAEgan implements a discriminator network $D$ in the latent space $Z$ trying to differentiate between samples drawn from $P_Z$ and samples drawn from $Q_Z$, essentially setting $\\mathcal{D}_Z(Q_Z,P_Z)=D(Q_Z,P_Z)$, and forcing $Q$ to learn to generate latent codes that fool the discriminator $D$. In addition to implementing the WAEgan, we implement a VAE and vanilla AE as well. We choose this approach because, to better understand the WAE and its benefits, it is important to consider WAE within the context of these two preceeding and well-established algorithms. This approach provides a more intuitive understanding of the results by demonstrating side-by-side comparisons of each algorithm applied to the popular MNIST (CITE) and FashionMNIST (CITE) datasets with convolutional nueral network (CNN) implementations in PyTorch. \n",
    "\n",
    "<a id=\"fig1\"></a>\n",
    "**Figure 1:** Conceptual comparison of AE reconstruction methods (after Tolsikhin, et.al 2018). All three algorithms map inputs $x \\in X$ to a latent code $z \\in Z$ and then attempt to reconstruct $\\hat{x}=G(z)$. The AE places no regularization penalty on $Z$, while the VAE and WAE use Kullbackâ€“Leibler divergence (KLD) and optimal transport cost respectively to penalize divergence of $Q_Z$ from the shape of the prior, $P_Z$. While KLD forces Q(Z|X=x) to match $P_Z$, the optimal transport cost enforces the continuous mixture $Q_z:=\\int Q(Z|X) dP_x$ to match $P_Z$.\n",
    "\n",
    "![alt text](https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/figure%201%20-%20reconstruction.png?raw=true \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"details\"></a>\n",
    "## Mathematics and algorithms\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we provide the mathematical detail and algorithmic differences between each method, paying extra attention to WAE and how it varies from VAE.\n",
    "\n",
    "**latex to include:**\n",
    "1. notational algorithms\n",
    "1. loss function detail\n",
    "1. mathematical representation of the reparameterization trick\n",
    "\n",
    "**images to include:**\n",
    "1. A small graphical representation of the reparameterization trick (small and simple node/edge plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mnist\"></a>\n",
    "## Comparing results on MNIST\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we specify the parameters used in our model and provide plots and metrics and written interpretation describing the training results and latent space representations of our algorithms on MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. Sample of 5 original MNIST images and corresponding decoded images for AE, VAE, and WAE on separate rows\n",
    "1. Latent space linear interpolation results of each model, pixel space vs AE vs VAE vs WAE on separate rows\n",
    "1. tSNE or PCA representation of pixel space vs latent space for each model to demonstrate differences\n",
    "1. table summarizing comparative loss (and if possible FID results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fmnist\"></a>\n",
    "## Comparing results on FashionMNIST\n",
    "[return to top](#top)\n",
    "\n",
    "Same as above for MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. same as above for MNIST, but probably smaller and with fewer examples if results demonstrate similar characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusions and further analysis\n",
    "[return to top](#top)\n",
    "\n",
    "Here we summarize our conclusions given MNIST and FMNIST, but also describe other dataset we may want to run as comparison (e.g. celeb faces for representation on a low manifold surface such a faces, RNA expression data for investigation of a novel application of WAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## References and Further Reading\n",
    "[return to top](#top)\n",
    "\n",
    "Cite the papers, repos, datasets, and blogs we used in our analysis, as well as any other resources we want to direct our readers toward\n",
    "\n",
    "1. VAE paper\n",
    "1. WAE paper\n",
    "1. PyTorch/resources implementation of VAE\n",
    "1. AE paper?\n",
    "1. MNIST\n",
    "1. FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix\"></a>\n",
    "## Appendices: PyTorch Implementation\n",
    "[return to top](#top)\n",
    "\n",
    "- The Appendix is where we lay out and run our PyTorch code, each model is separated among sub-appendices\n",
    "- We should output our most important plots to png (saved on GitHub) so we can display them via markdown img link at the appropriate locations in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dim_h'] = 40            # factor controlling size of hidden layers\n",
    "args['n_channel'] = 1         # number of channels in the input data (MNIST is 1, aka greyscale)\n",
    "args['n_z'] = 20              # number of dimensions in latent space. \n",
    "args['sigma'] = 1.0           # variance in n_z\n",
    "args['lambda'] = 0.01         # hyper param for weight of discriminator loss\n",
    "args['lr'] = 0.0002           # learning rate for Adam optimizer\n",
    "args['epochs'] = 50           # how many epochs to run for\n",
    "args['batch_size'] = 256      # batch size for SGD\n",
    "args['save'] = False          # save weights at each epoch of training if True\n",
    "args['train'] = False         # train networks if True, else load networks from saved weights\n",
    "args['dataset'] = 'mnist'     # specify which dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Dataset\n",
    "if args['dataset'] == 'mnist':\n",
    "    trainset = datasets.MNIST(\n",
    "        root='./MNIST/',\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    testset = datasets.MNIST(\n",
    "        root='./MNIST/',\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "elif args['dataset'] == 'fmnist':\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root='./FMNIST/',\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root='./FMNIST/',\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ae\"></a>\n",
    "### Appendix A: Auto-encoder \n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class AE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters organized according to the popular DCGAN (Radford et. al., 2015) framework, excellent for image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class AE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models, and set to train mode\n",
    "ae_encoder, ae_decoder = AE_Encoder(args), AE_Decoder(args)\n",
    "\n",
    "if args['train']:\n",
    "    # specify loss (mean squared error of image reconstruction)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # use the Adam optimizer, it's always a good choice\n",
    "    enc_optim = torch.optim.Adam(ae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(ae_decoder.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            ae_encoder.train()\n",
    "            ae_decoder.train()\n",
    "\n",
    "            ae_encoder.zero_grad()\n",
    "            ae_decoder.zero_grad()\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            z_hat = ae_encoder(images)\n",
    "            x_hat = ae_decoder(z_hat)\n",
    "            train_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "            train_recon_loss.backward()\n",
    "\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # Run validation set\n",
    "        ae_encoder.eval()\n",
    "        ae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat = ae_encoder(images)\n",
    "            x_hat = ae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "        if args['save']:\n",
    "            save_path = './save/AE_{}-epoch_{}.pth'\n",
    "            torch.save(ae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(ae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {}\\n\\t\\t\\tTest Reconstruction Loss: {}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            train_recon_loss.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "else:\n",
    "    # load encoder and decoder weights from checkpoint\n",
    "    enc_checkpoint = torch.load('save/AE_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    ae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/AE_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    ae_decoder.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "### Appendix B: Variational auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc1 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "        self.fc2 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "        \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = h.squeeze()\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models, and set to train mode\n",
    "vae_encoder, vae_decoder = VAE_Encoder(args), VAE_Decoder(args)\n",
    "\n",
    "if args['train']:\n",
    "    # specify loss (mean squared error of pixel by pixel image reconstruction)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # use the Adam optimizer, it's always a good choice\n",
    "    enc_optim = torch.optim.Adam(vae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(vae_decoder.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            vae_encoder.train()\n",
    "            vae_decoder.train()\n",
    "\n",
    "            vae_encoder.zero_grad()\n",
    "            vae_decoder.zero_grad()\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            z_hat, mu, logvar = vae_encoder(images)\n",
    "            x_hat = vae_decoder(z_hat)\n",
    "            \n",
    "            BCE = nn.functional.binary_cross_entropy(\n",
    "                x_hat.view(-1,784), \n",
    "                images.view(-1, 784), \n",
    "                reduce=False\n",
    "            ).sum()\n",
    "            \n",
    "            KLD = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            \n",
    "            ELBO = BCE - KLD\n",
    "            ELBO.backward()\n",
    "                        \n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # Run validation set\n",
    "        vae_encoder.eval()\n",
    "        vae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat, mu, logvar = vae_encoder(images)\n",
    "            x_hat = vae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images) # maybe change to BCE?\n",
    "\n",
    "        if args['save']:\n",
    "            save_path = './save/VAE_{}-epoch_{}.pth'\n",
    "            torch.save(vae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(vae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {} \\tKLD:{}\\n\\t\\t\\tTest Reconstruction Loss: {}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            BCE.data.item(),\n",
    "            KLD.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "else:\n",
    "    # load encoder and decoder weights from checkpoint\n",
    "    enc_checkpoint = torch.load('save/VAE_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    vae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/VAE_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    vae_decoder.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wae\"></a>\n",
    "### Appendix C: Wasserstein auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class WAE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WAE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class WAE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WAE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# define the descriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # main body of discriminator, returns [0,1]\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "    \n",
    "# control which parameters are frozen / free for optimization\n",
    "def free_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def frozen_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate discriminator model, and restart encoder and decoder, for fairness. Set to train mode, etc\n",
    "wae_encoder, wae_decoder, discriminator = WAE_Encoder(args), WAE_Decoder(args), Discriminator(args)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if args['train']:\n",
    "    enc_optim = torch.optim.Adam(wae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(wae_decoder.parameters(), lr = args['lr'])\n",
    "    dis_optim = torch.optim.Adam(discriminator.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "    dis_scheduler = torch.optim.lr_scheduler.StepLR(dis_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    # one and -one allow us to control descending / ascending gradient descent\n",
    "    one = torch.Tensor([1])\n",
    "    \n",
    "    for epoch in range(args['epochs']):\n",
    "\n",
    "        # train for one epoch -- set nets to train mode\n",
    "        wae_encoder.train()\n",
    "        wae_decoder.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            # zero gradients for each batch\n",
    "            wae_encoder.zero_grad()\n",
    "            wae_decoder.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # ======== Train Discriminator ======== #\n",
    "\n",
    "            # freeze auto encoder params\n",
    "            frozen_params(wae_decoder)\n",
    "            frozen_params(wae_encoder)\n",
    "\n",
    "            # free discriminator params\n",
    "            free_params(discriminator)\n",
    "\n",
    "            # run discriminator against randn draws\n",
    "            z = torch.randn(images.size()[0], args['n_z']) * args['sigma']\n",
    "            d_z = discriminator(z)\n",
    "\n",
    "            # run discriminator against encoder z's\n",
    "            z_hat = wae_encoder(images)\n",
    "            d_z_hat = discriminator(z_hat)\n",
    "\n",
    "            d_z_loss = args['lambda']*torch.log(d_z).mean()\n",
    "            d_z_hat_loss = args['lambda']*torch.log(1 - d_z_hat).mean()\n",
    "\n",
    "            # formula for ascending the descriminator -- -one reverses the direction of the gradient.\n",
    "            d_z_loss.backward(-one)\n",
    "            d_z_hat_loss.backward(-one)\n",
    "\n",
    "            dis_optim.step()\n",
    "\n",
    "            # ======== Train Generator ======== #\n",
    "\n",
    "            # flip which networks are frozen, which are not\n",
    "            free_params(wae_decoder)\n",
    "            free_params(wae_encoder)\n",
    "            frozen_params(discriminator)\n",
    "\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            # run images\n",
    "            z_hat = wae_encoder(images)\n",
    "            x_hat = wae_decoder(z_hat)\n",
    "\n",
    "            # discriminate latents\n",
    "            z_hat2 = wae_encoder(Variable(images.data))\n",
    "            d_z_hat = discriminator(z_hat2)\n",
    "\n",
    "            # calculate reconstruction loss\n",
    "            # WAE is happy with whatever cost function, let's use BCE\n",
    "            BCE = nn.functional.binary_cross_entropy(\n",
    "                x_hat.view(-1,784), \n",
    "                images.view(-1, 784), \n",
    "                reduce=False\n",
    "            ).mean()\n",
    "            \n",
    "            # calculate discriminator loss\n",
    "            d_loss = args['lambda'] * (torch.log(d_z_hat)).mean()\n",
    "            \n",
    "            # we keep the BCE and d_loss on separate graphs to increase efficiency in pytorch\n",
    "            BCE.backward(one)\n",
    "            # -one reverse the direction of the gradient, minimizing BCE - d_loss\n",
    "            d_loss.backward(-one)\n",
    "\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # test on test set\n",
    "        wae_encoder.eval()\n",
    "        wae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat = wae_encoder(images)\n",
    "            x_hat = wae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "        \n",
    "        if args['save']:\n",
    "            save_path = './save/WAEgan_{}-epoch_{}.pth'\n",
    "            torch.save(wae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(wae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "            torch.save(discriminator.state_dict(), save_path.format('discriminator', epoch))\n",
    "\n",
    "        # print stats after each epoch\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {} d loss: {}, \\n\\t\\t\\tTest Reconstruction Loss:{}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            BCE.data.item(),\n",
    "            d_loss.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "        \n",
    "else:\n",
    "    enc_checkpoint = torch.load('save/WAEgan_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    wae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/WAEgan_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    wae_decoder.load_state_dict(dec_checkpoint)\n",
    "    \n",
    "    dec_checkpoint = torch.load('save/WAEgan_discriminator-best_{}.pth'.format(args['dataset']))\n",
    "    discriminator.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "### Appendix D: Plotting functions\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BOTH ALGORITHMS SHOULD PROBABLY BE REWRITTEN USING LATEX IN A WAY THAT TAKES UP LESS VERTICAL SPACE**\n",
    "\n",
    "<a id=\"algo2\"></a>\n",
    "**Algorithm 2:** Wassertein auto-encoder with GAN-based penalty (WAE-GAN) pseudocode\n",
    "\n",
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$, and latent discriminator $D_{\\gamma}$.\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{z_1, \\dotsc , z_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $D_{\\gamma}$ by ascending:\n",
    "$$\\frac{\\lambda}{n}\\sum_{i=1}^n log \\; D_{\\gamma}(z_i) + log (1-D_{\\gamma}(\\tilde{z}_i))$$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i)) - \\lambda \\cdot log\\;D_{\\gamma}(\\tilde{z}_i)$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"algo1\"></a>\n",
    "**Algorithm 1:** Variational auto-encoder pseudocode for computing a stochastic graient using the estimator\n",
    "\n",
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "\n",
    "> Initialize the parameters for the encoder $Q_{\\phi}$ and decoder $G_{\\theta}$\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{\\epsilon_1, \\dotsc , \\epsilon_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i))$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
